# Escalar y distribuir {#sec-chap-scalingup}

{{< include common_setup.qmd >}}
```{bash}
#| echo: false
# Remove the database in case it exists to avoid 'table X already exists' error
rm -f mydb.db mydb.sqlite
```

**Resumen.**
 A lo largo de este libro, hemos estado trabajando con ejemplos consistentes en un código creado para realizar un análisis específico de 
conjuntos de datos de tamaño modesto. Pero, seguramente, quieras ampliarlo llegado el momento. Tanto para que otros puedan aplicar tu 
código a sus datos, como para poder utilizar tus propios análisis en conjuntos de datos más grandes y complejos. O puede que necesites 
ejecutar análisis que tu ordenador no puede ejecutar. Este capítulo se ocupa de estos casos y te muestra algunas técnicas que resultan 
más útiles cuanto más grandes son tus proyectos.

**Palabras clave.** Bases de datos, computación en nube, contenerización, código fuente, control de versiones

**Objetivos:**

-  Ser capaz de ampliar sus análisis
-  Saber cuándo utilizar bases de datos
-  Saber cuándo utilizar la computación en nube
-  Saber sobre código fuente distribuido y contenedores

::: {.callout-note icon=false collapse=true}

En este capítulo, ofrecemos una breve visión general de las técnicas para escalar los análisis computacionales. En concreto, presentamos 
las bases de datos SQL y noSQL, las plataformas de computación en la nube, los sistemas de control de versiones y los contenedores 
Docker
:::

## Almacenamiento de datos en bases de datos SQL y NoSQL {#sec-databases}

### Cuándo utilizar una base de datos {#sec-whendb}

En este libro, hasta ahora hemos almacenado nuestros datos en ficheros. Por ello, antes de cubrir la amplia gama de métodos para el 
análisis computacional, discutimos algunos aspectos básicos del manejo de archivos (Capítulo [-@sec-chap-filetodata]). Lo más probable es 
que aún no hayas experimentado ningún problema importante (aparte de problemas ocasionales con codificaciones no estándar, o confusión 
sobre los delimitadores en un archivo csv). Pero no olvides que los ejemplos que utilizamos eran de tamaño bastante modesto: normalmente, 
se trataba de un puñado de archivos csv. A excepción del caso de los enormes conjuntos de datos de clasificación de imágenes, lo máximo 
con lo que hemos tenido que lidiar son los 50.000 archivos de texto del conjunto de datos de reseñas de películas IMDB.

En particular, al cargar tus datos en un marco de datos, lo que has hecho es copiar todos los datos del disco a la memoria[^1]. Pero, 
¿qué ocurre si queremos ampliar un poco nuestros análisis (@Trilling2018b?)? Tal vez queramos crear una colección de datos más grande, 
tal vez incluso compartirla con múltiples miembros del equipo, buscar y filtrar nuestros datos, o recopilarlos durante un período de 
tiempo más largo. En cualquiera de estos casos, un ejemplo puede ilustrar los problemas que pueden surgir:

Imagina que haces algo de *web scraping* (Capítulo [-@sec-chap-scraping]) que va más allá de unos pocos miles de textos. Tal vez quieras 
visitar sitios de noticias relevantes de forma regular (digamos, una vez por hora) y recuperar todo lo que sea nuevo. ¿Cómo se almacenan 
esos datos? Quizás, añadiéndolo todo a un enorme archivo csv, pero este archivo crecería rápidamente hasta el punto en que ya no podrías 
cargarlo en memoria. Además, podrías correr el riesgo de corromper el archivo si algo sale mal en uno de tus intentos de ampliarlo. Otra 
opción sería escribir cada artículo en un archivo nuevo e independiente: sería más a prueba de fallos, pero necesitarías diseñar una buena 
forma de organizar los datos. En realidad, idear un método para buscar y encontrar archivos relevantes sería un proyecto en sí mismo.

Por suerte, puedes externalizar todos estos problemas a una base de datos instalada en tu propio ordenador o en un servidor (en ese caso, 
asegúrate de que está bien protegido). En el ejemplo, el *scraper*, que se ejecuta una vez por hora, enviaría los datos extraídos a la base 
de datos en lugar de a un archivo, y la base de datos se encargaría de almacenarlos. Cuando quisieras recuperar un subconjunto de tus 
artículos para analizarlos, podrías enviar una consulta a la base de datos y leerlos desde ella. Tanto Python como R ofrecen integración 
para múltiples bases de datos de uso común. Incluso es posible obtener directamente los resultados de dicha consulta a la base de datos 
en forma de marco de datos.

Podemos distinguir dos categorías principales de bases de datos que son especialmente relevantes para nosotros (véase también @Gunther2018): 
bases de datos relacionales (o bases de datos SQL) y bases de datos noSQL. En sentido estricto, SQL ("lenguaje de consulta estructurado") 
es un lenguaje de consulta para bases de datos, pero está tan extendido que se utiliza casi como sinónimo de bases de datos relacionales. 
Aunque existen desde hace ya 50 años ([@Codd1970]), las bases de datos relacionales siguen siendo muy potentes y se utilizan mucho. 
Constan de varias tablas vinculadas por columnas compartidas (claves). Por ejemplo, se puede imaginar una tabla con los pedidos realizados 
en una tienda online que tenga una columna cliente-id, y otra tabla con direcciones, información de facturación y nombres para cada 
cliente-id. Utilizando operaciones de filtrado y unión (como en el Capítulo [-@sec-chap-datawrangling], pero directamente sobre la base 
de datos), se puede recuperar fácilmente la información sobre el lugar al que debe enviarse el pedido. Una gran ventaja de una base de 
datos relacional de este tipo es que, si un cliente hace 100 pedidos, no necesitamos almacenar su dirección 100 veces, sino solo una, lo 
que no solo es más eficiente en términos de almacenamiento, sino que también evita incoherencias en los datos.

A diferencia de las bases de datos SQL, las bases de datos noSQL no se basan en tablas, sino que utilizan conceptos como "documentos" o 
pares clave-valor, muy parecidos a los diccionarios de Python o los archivos JSON. Este tipo de bases de datos son especialmente 
interesantes cuando los datos están menos estructurados. Si no todos tus casos tienen las mismas variables, o si el contenido no está 
bien definido (digamos que no sabes exactamente en qué formato se escribirá la fecha de publicación en un sitio de noticias), o si la 
estructura de los datos puede cambiar con el tiempo, entonces es difícil o imposible dar con una buena estructura de tablas para una base 
de datos SQL. Por eso, en muchos contextos de "*big data*" se utilizan bases de datos noSQL, ya que (dependiendo de su configuración) 
aceptarán de buen grado casi cualquier tipo de contenido que se vuelque en ellas. Por supuesto, esto se consigue a costa de renunciar a 
las ventajas de las bases de datos SQL, como evitar las incoherencias. Pero, a menudo, es preferible almacenar los datos primero y 
limpiarlos después, en lugar de arriesgarse a que la recopilación de datos falle por haber aplicado una estructura demasiado estricta. 
Además, hay muchas bases de datos noSQL muy rápidas en la búsqueda de texto completo, algo para lo que las bases de datos SQL, en general, 
no están optimizadas.

A pesar de todas estas diferencias, tanto las bases de datos SQL como las noSQL pueden desempeñar el mismo papel en el análisis 
computacional de la comunicación. Ambas te ayudan a centrarte en la recopilación y el análisis de datos sin necesidad de idear una forma 
ingeniosa de almacenarlos. Ambos permiten una búsqueda y un filtrado mucho más eficientes que los que podrías diseñar por tu cuenta. Todo 
esto resulta especialmente interesante cuando el conjunto de datos crece demasiado como para caber en la memoria, pero también cuando los 
datos cambian continuamente, como cuando se añaden nuevos datos durante el *scraping*.

### Elegir la base de datos adecuada {#sec-rightdb}

Elegir la base de datos adecuada no siempre es fácil, y tiene muchas consecuencias en la forma en que podrás llevar a cabo tus análisis. 
Como explican @Gunther2018, no se trata de una elección puramente técnica, sino que afecta a su flujo de trabajo científico-social. 
¿Deseas imponer una estructura específica desde el principio o prefieres recopilarlo todo y limpiarlo después? ¿Dónde está el equilibrio 
entre evitar cualquier incoherencia y arriesgarte a desechar demasiada información?

Siendo conscientes de que a menudo hay muchas opciones válidas, y a riesgo de simplificar demasiado las cosas, intentaremos orientarte 
sobre qué bases de datos elegir a través de algunas preguntas orientativas.

**¿Cómo están estructurados tus datos?**
Pregúntate: ¿Puedo organizar mis datos en un conjunto de tablas relacionales? Por ejemplo, piensa en los datos de visionado de televisión: 
puede haber una tabla que dé información sobre cuándo se encendió el televisor y qué canal se vio y por qué id de usuario. Una segunda 
tabla puede servir para asociar características personales como la edad y el sexo al identificador de usuario. Y una tercera tabla puede 
servir para asociar las marcas de tiempo a detalles sobre un programa concreto emitido en ese momento. Si tus datos tienen este aspecto, 
pregúntate: ¿Puedo determinar de antemano las columnas y los tipos de datos de cada columna? Si es así, una base de datos SQL como 'MySQL', 
'PostgreSQL' o 'MariaDB' es, probablemente, lo que estás buscando. Si, por el contrario, no puedes determinar dicha estructura a priori, 
si crees que la estructura de tu información cambiará con el tiempo o si está muy desordenada, entonces puede que necesites un enfoque 
noSQL más flexible, como 'MongoDB' o 'ElasticSearch'.

**¿Qué importancia tiene para ti la búsqueda de texto completo?**
Las bases de datos SQL pueden manejar tanto tipos de datos numéricos como de texto, pero no suelen estar optimizadas para estos últimos. 
Manejan bien cadenas cortas (como nombres de usuario, direcciones, etc.), pero si lo que te interesa es la búsqueda de texto completo, no 
son la herramienta adecuada. Esto es especialmente cierto si quieres hacer búsquedas difusas en las que, por ejemplo, también se 
encuentren documentos que contengan el plural de una palabra que has buscado en singular. Las bases de datos de, por ejemplo, artículos 
de noticias, tuits, transcripciones de discursos u otros documentos se consultan mucho mejor en una base de datos como 'ElasticSearch'.

**¿Cómo de flexible tiene que ser?**
En las bases de datos relacionales, es relativamente difícil cambiar la estructura a posteriori. En cambio, una base de datos noSQL no 
tiene ningún problema en añadir un nuevo documento que contenga claves que antes no existían. No se presupone que todos los documentos 
contengan las mismas claves. Por lo tanto, si es difícil saber de antemano qué "columnas" o "claves" pueden representar mejor tus datos, 
deberías mantenerte alejado de las bases de datos SQL. En particular, si piensas ampliar gradualmente tus datos y utilizarlos a largo 
plazo para su reutilización, incluso por varios equipos, la flexibilidad de una base de datos noSQL puede resultarte perfecta.

### Un breve ejemplo utilizando SQLite {#sec-sqlite}

Instalar un servidor de base de datos como 'mysql', 'mariadb' (una bifurcación de código abierto de 'mysql'), 'MongoDB', o 'Elasticsearch' 
no es realmente difícil (de hecho, puede que ya venga incluido en tu sistema operativo), pero la configuración exacta puede variar mucho 
dependiendo de tu ordenador y tus necesidades. Y lo que es más importante, especialmente si almacenas datos confidenciales en tu base de 
datos, tendrás que pensar en la autenticación, los roles, etc., aunque todo ello más allá del alcance de este libro.

Por suerte, existe una solución intermedia entre almacenar los datos en archivos que debes gestionar tú mismo y configurar un servidor de 
bases de datos, local o remotamente. La biblioteca 'SQlite' ofrece un motor de base de datos autónomo: básicamente, permite almacenar 
toda una base de datos en un archivo e interactuar con ella mediante el lenguaje de consulta SQL. Tanto R como Python ofrecen múltiples 
formas de interactuar directamente con archivos 'sqlite' (Ejemplo [-@exm-sqlite]). Esto te da acceso a una gran funcionalidad de inmediato: 
después de todo, ahora puedes emitir (casi) cualquier comando SQL, incluyendo (y quizás lo más importante) comandos para filtrar, unir y 
agregar datos. O puedes considerar guardar inmediatamente cada dato que obtengas de una API o de un *webscraper* (Capítulo 
[-@sec-chap-scraping]) sin arriesgarte a perder ningún dato si las conexiones se interrumpen o el *scraping* falla a medio camino.

::: {.callout-note appearance="simple" icon=false}

::: {#exm-sqlite}
SQLite te ofrece funciones de base de datos sin necesidad de configurar un servidor de bases de datos como mysql

::: {.panel-tabset}
## Código Python
```{python sqlite-python}
import pandas as pd
import sqlite3

# Carga un marco de datos
url = "https://cssbook.net/d/gun-polls.csv"
d = pd.read_csv(url)

# conecta a una base de datos SQLite
conn = sqlite3.connect("mydb.db")
# store the df as table "gunpolls" in the database
d.to_sql("gunpolls", con=conn)

# ejecuta una consulta en la base de datos SQLite
sql = """SELECT support, pollster 
         FROM gunpolls LIMIT 5;"""
d2 = pd.read_sql_query(sql, conn)
# cierra la conexión
conn.close()
d2

```
## Código R
```{r sqlite-r}
library(tidyverse)
library(RSQLite)

# Carga un marco de datos
url = "https://cssbook.net/d/gun-polls.csv"
d = read_csv(url)

# conecta a una base de datos SQLite
mydb = dbConnect(RSQLite::SQLite(), "mydb.sqlite")
# store the df as table "gunpolls" in the database
dbWriteTable(mydb, "gunpolls", d)

# ejecuta una consulta en la base de datos SQLite
sql = "SELECT support, pollster 
       FROM gunpolls LIMIT 5;"
d2 = dbGetQuery(mydb, sql)
d2
# cierra la conexión
dbDisconnect(mydb)

```
:::
:::
:::

Por supuesto, 'SQlite' no puede ofrecerte el mismo rendimiento que una instalación "real" de 'mysql' (o similar). Por lo tanto, si tu 
proyecto crece, o si tienes muchas operaciones de lectura o escritura por segundo, puede que tengas que cambiar de método en algún 
momento. Pero como puedes ver en el Ejemplo [-@exm-sqlite], Python y R no se preocupan realmente por el servidor: todo lo que necesitas 
hacer es cambiar la conexión conn para que se dirija a tu nueva base de datos en lugar de al archivo 'sqlite'.

## Utilización de servicios en la nube {#sec-cloudcomputing}

A lo largo de este libro, hemos asumido que todas las tareas pueden realizarse en tu propio ordenador. Y a menudo, es la mejor opción: 
mes recomendable mantener una copia local de los datos, y puede ser la apuesta más segura por razones éticas y legales (cuando se trabaja 
con datos sensibles, tienes que tener claro lo que estás haciendo antes de transferirlos a otro lugar).

Aun así, una vez que se amplía el proyecto, pueden surgir problemas (véase @Trilling2018b): 

 -  Múltiples personas necesitan trabajar en los mismos datos
 -  Su conjunto de datos es demasiado grande para caber en el disco
 -  No tienes suficiente RAM o potencia de procesamiento
 -  Ejecutar un proceso lleva demasiado tiempo (por ejemplo, entrenar un modelo durante varios días), el programa necesita ejecutarse en intervalos continuos (por ejemplo, recuperar artículos de noticias una vez por hora) o, sencillamente, necesitas tu ordenador para otras cosas.

Llegados a este punto, necesitas empezar a mover tu proyecto a algún servidor remoto. En términos generales, podemos considerar cuatro 
escenarios.

1. Primero, un servicio en la nube que solo te permite ejecutar código: en este caso, basta con enviar el código para que se ejecute. No tienes control total, no puedes configurar tu propio sistema, pero tampoco tienes que hacer ninguna administración. 
2. La segunda opción es un servidor dedicado: Tú mismo (o tu universidad) puedes comprar un servidor físico dedicado para realizar análisis computacionales de ciencias sociales. Por el lado bueno, esto te da un control total, pero tampoco es muy flexible: haces una gran inversión una vez, y si resulta que necesitas más (o menos) recursos, probablemente no puedas cambiarlo. 
3. Si estas opciones no te sirven, puedes recurrir a una máquina virtual (VM) en una plataforma de computación en la nube. A efectos prácticos, puedes hacer lo mismo que en la opción anterior, con la diferencia crucial de que los recursos están alquilados. Si necesitas más, simplemente alquilas más; y, cuando hayas terminado, paras la máquina. 
4. Por último, un conjunto de máquinas para ejecutar tareas complejas mediante computación paralela puede ser una buena opción. Si quieres procesar grandes cantidades de información (piensa en datos de imagen o vídeo) y un modelado sofisticado (como el aprendizaje profundo) puede que necesites distribuir el cómputo entre varios ordenadores diferentes al mismo tiempo.

Un ejemplo de la primera opción es 'Google Colab'. Aunque facilita el uso compartido y la ejecución de cuadernos Jupyter, el nivel 
gratuito que hemos utilizado hasta ahora no resuelve ninguno de los problemas de escalabilidad comentados. Sin embargo, Google Colab 
también tiene una versión Pro de pago, en la que se puede utilizar hardware adicional (como GPU, TPU o memoria extra) del que quizás no 
dispongas en tu propio ordenador. Esto la convierte en una solución atractiva para realizar proyectos (por ejemplo, que impliquen redes 
neuronales que consuman muchos recursos) que de otro modo no serían posibles.

Sin embargo, a menudo esto no es suficiente. Por ejemplo, puede que quieras ejecutar una base de datos (Sección [-@sec-databases]) o 
definir lo que se conoce como 'cron job', que ejecuta un *script* específico (por ejemplo, un *web scraper*) a intervalos definidos. Aquí, 
las opciones 2 y 3 entran en juego: aunque la más realista para la mayoría de los principiantes, es la opción 3.

Existen diferentes proveedores para instalar máquinas virtuales en la nube, siendo probablemente los más conocidos 'Amazon Web Services' 
(AWS) y 'Microsoft Azure'. Algunas universidades o proveedores (nacionales) de infraestructuras de investigación también ofrecen 
computación de alto rendimiento en la nube. Aunque la forma específica de configurar una máquina virtual propia en una infraestructura de 
este tipo varía, los procesos son más o menos similares: se seleccionan las especificaciones técnicas, como el número de núcleos de CPU y 
la cantidad de memoria que se necesita, se adjunta algo de almacenamiento y se selecciona una imagen de disco con un sistema operativo, 
prácticamente siempre alguna distribución de Linux ([@fig-createvm], [@fig-createvm2]). Tras un par de minutos, tu máquina estará lista 
para funcionar.

![Creación de una máquina virtual en Microsoft Azure](img/vmazure.png){#fig-createvm}

![Creación de una máquina virtual en una plataforma de computación en nube universitaria utilizando OpenNebula.](img/vmopennebula.png){#fig-createvm2}

Aunque configurar una máquina de este tipo es fácil, se requieren ciertos conocimientos para administrarla de forma responsable y segura, 
sobre todo para evitar accesos no autorizados.

Imagina que tienes un *script* 'myscript.py' que tarda un par de días en ejecutarse. Puedes usar la herramienta 'scp' para copiarlo en 
una máquina virtual, conectarte a ella usando 'ssh', y ejecutar (¡en la máquina virtual!) el *script* utilizando una herramienta como 
'nohup' o 'screen', que lo iniciará y lo mantendrá funcionando. (Figura [-@fig-ssh]). Hecho esto, puedes desconectarte con seguridad y tu 
máquina virtual en la nube seguirá haciendo su trabajo. Lo único que tienes que hacer es recoger los resultados una vez haya finalizado 
el *script*, aunque sea un par de semanas más tarde. También puedes añadir tu *script* al 'crontab' (búscalo en Google), que lo ejecutará 
automáticamente a intervalos determinados.

![ Ejecución de un *script* en una máquina virtual. Observa que los dos primeros comandos se emiten en la máquina local ('damian-thinkpad') y el siguiente comando en la máquina remota ('packer-ubuntu-16').](img/ssh.png){#fig-ssh}

Sin embargo, puede que quieras darte algún lujo extra. A mucha gente le gusta configurar bases de datos (Sección [-@sec-databases]) y 
JupyterHub (permite a usuarios como tus colegas conectarse a través de su navegador web con tu servidor y ejecutar sus propios cuadernos 
Jupyter en la máquina). No olvides encriptar adecuadamente todas las conexiones, por ejemplo, usando 'letsencrypt'.

Por último, la opción 4 debe seleccionarse cuando la escala de los datos y la complejidad de las tareas no puedan desplegarse en un único 
servidor o máquina virtual. Por ejemplo, construir un modelo de clasificación entrenando una red neuronal convolucional compleja y 
profunda con millones de imágenes y actualizar este modelo constantemente puede requerir el uso de diferentes ordenadores al mismo 
tiempo. En realidad, en los ordenadores modernos con múltiples núcleos o procesadores, normalmente se ejecuta la computación en paralelo 
dentro de una única máquina. Pero cuando se trabaja a escala es probable que sea necesario establecer una infraestructura de distintos 
ordenadores como la de computación de malla o un clúster de ordenadores.

Los servicios en la nube (AWS, Microsoft Azure, etc.) o las infraestructuras científicas (por ejemplo, los superordenadores) ofrecen la 
posibilidad de configurar estas arquitecturas de forma remota. Por ejemplo, en un clúster de ordenadores se puede configurar un grupo de 
máquinas virtuales, donde una actuará como principal (*main*) y las demás como trabajadoras (*workers*). Con esta lógica, el ordenador 
principal puede distribuir el almacenamiento y el análisis de datos entre los trabajadores y luego reunir los resultados: véanse, por 
ejemplo, los enfoques 'MapReduce' o 'Resilient Distributed Dataset' (RDD) utilizados por el software de código abierto Apache Hadoop y 
Apache Spark, respectivamente. Para un ejemplo concreto de computación paralela en el análisis computacional de la comunicación se puede 
echar un vistazo a la implementación del análisis supervisado distribuido de sentimientos, en el que uno de los autores de este libro 
desplegó la clasificación supervisada de textos en Apache Spark y conectó esta infraestructura con el análisis en tiempo real de tuits, 
utilizando para ello Apache Kafka, con el fin de realizar análisis de datos en directo @calderon2019distributed.

Estas arquitecturas para el procesamiento en paralelo aumentarán significativamente tu capacidad de cálculo para problemas de *big data*, 
pero la implementación inicial consumirá tiempo y (la mayoría de las veces) dinero, razón por la que debes pensar de antemano si existe 
una solución más sencilla (como una única pero potente máquina) antes de implementar una infraestructura más compleja en tu análisis.

## Publicar tu fuente {#sec-publishingsource}

Ya en la Sección [-@sec-practices], introdujimos brevemente la idea de los protocolos de control de versiones, como 'git', y el 
repositorio git en línea más conocido, 'GitHub'. Hay otros, como 'Bitbucket', pero el por qué utilizar uno u otro no es relevante en 
este momento.  Para proyectos pequeños, ya es una buena idea usar el control de versiones para poder volver siempre a versiones 
anteriores, pero en cuanto empiezas a trabajar con varias personas en un proyecto, se vuelve indispensable.

Por ejemplo, es posible trabajar en múltiples ramas, diferentes versiones del código que posteriormente puedan volver a fusionarse. De 
este modo, es posible desarrollar nuevas características sin interferir con la versión principal del código. Hay multitud de tutoriales 
sobre git disponibles en Internet, y recomendamos encarecidamente utilizar git desde el principio del proyecto, ya sea tu tesis de 
licenciatura, máster o doctorado, un trabajo o una herramienta que quieras crear.

En el análisis computacional de la comunicación, cada vez es más habitual publicar todo tu código fuente junto con un artículo, aunque es 
importante tener en cuenta las restricciones éticas y legales (@VanAtteveldt2019). Utilizar una plataforma de control de versiones como 
GitHub desde el principio facilita las cosas: al publicar el artículo, lo único que tienes que hacer es establecer el acceso de tu 
repositorio como "público" (en caso de que antes fuera privado), añadir un archivo 'README.md' (en caso de que no lo hayas hecho antes) y, 
preferiblemente, obtener un identificador persistente, un doi para tu código (consulta 
'[guides.github.com/activities/citable-code/](https://guides.github.com/activities/citable-code/))'. No olvides añadir una licencia a tu 
código, como MIT, GPL o Apache. Todas ellas tienen implicaciones específicas sobre lo que otros pueden o no pueden hacer con tu código 
(por ejemplo, si puede utilizarse con fines comerciales o si los derivados deben publicarse también bajo la misma licencia). Sea cual sea 
tu elección, es importante que la hagas, ya que, de lo contrario, puede que no sea posible (legalmente) utilizar tu código. Si tu código 
pertenece a un artículo específico, sugerimos que organices tu repositorio como un "compendio de investigación", integrando tanto el 
código como los datos. @compendium proporcionan una plantilla y herramientas para crear uno fácilmente[^2].

En casi todos los casos, tu código dependerá de bibliotecas escritas por otros, que están disponibles gratuitamente. Por lo tanto, es 
justo ser igual de generosos y asegurarnos de que cualquier código que hayamos escrito y que pueda ser útil para otros, también esté 
disponible.

Al igual que en el caso de un compendio de investigación para un artículo específico, la publicación de código fuente para una 
reutilización más genérica también comienza con un repositorio de GitHub. De hecho, tanto R (con 'devtools') como Python (a través de 
'pip') pueden instalar paquetes directamente desde GitHub. Para asegurarte de que tu código puede instalarse como un paquete, tienes que 
seguir instrucciones específicas sobre cómo nombrar los archivos, cómo estructurar tu directorio, etc. (consulta 
'packaging.python.org/tutorials/packaging-projects/' y 'r-pkgs.had.co.nz/').

Independientemente de estas instrucciones técnicas específicas, puedes asegurarte desde el principio de que tu código es fácilmente 
reutilizable. La siguiente lista de comprobación puede ayudarte a ello:

-  No sobre-codifiques valores. En lugar de utilizar "myoutputfile.csv" o 50 en el script, crea constantes como OUTPUTFILE="myoutputfile" y NUMBER_OF_TOPICS=50 al principio del script, y utiliza estas variables en lugar de los valores más adelante. O, mejor aún, deja que el usuario proporcione estos argumentos como argumentos de línea de comandos o a través de un archivo de configuración.
-  Utiliza funciones. En lugar de escribir largos scripts que se ejecuten desde la primera línea hasta la última en ese orden, estructura el código en diferentes funciones que cumplan una tarea específica cada una y, por tanto, puedan reutilizarse. Si te encuentras copiando y pegando código, lo más probable es que puedas escribir una función en su lugar.
-  Documenta tu código. Utiliza docstrings (Python) o comentarios (R) para dejar claro qué hace cada función.

## Distribuir tu software como contenedor {#sec-container}

Cuando publiques tu software, puedes pensar en múltiples grupos de usuarios. Algunos pueden estar interesados en desarrollar tu código. 
A otros les dará igual el código y solo querrán que el programa funcione. Y muchos otros estarán en algún punto intermedio.

Una vez que tu código se vuelve más complejo y tiene más dependencias, publicar únicamente el código fuente (Sección 
[-@sec-publishingsource]) puede ser una carga para aquellos que solo quieren que tu código "se ejecute". Imagina un escenario en el que 
tu software requiere una versión específica de Python o R y/o algunas librerías muy específicas (o incluso incompatibles) que no quieres 
obligar al usuario a instalar.

O puede que tu posible usuario ni siquiera conozca R o Python.

Para estos casos, la solución son los llamados “contenedores”, cuya plataforma más destacada es Docker. Imagínatelo como una máquina 
virtual minimalista que incluye todo lo necesario para ejecutar tu software. Desde el exterior, nada de eso es visible: solo un puerto de 
red al que conectarse o una línea de comandos con la que interactuar, según lo que elijas.

El software en contenedores que utiliza Docker se distribuye como una “*Docker image*” (imagen de Docker). Puedes crear una imagen tú 
mismo, pero también se puede distribuir enviándola a un registro, como el Docker Hub. Si publicas tu software de esta manera, el usuario 
final no tiene que hacer nada más que instalar Docker y ejecutar el comando 'docker run nameofyourimage' (incluso se descargará 
automáticamente si es necesario). También existen versiones GUI de Docker, lo que reduce aún más el umbral para algunos grupos de 
usuarios finales.

Ilustremos el flujo de trabajo típico con un *toy example*. Imagina que escribes el siguiente *script*, 'myscript.py':

```{python}
#| eval: false
import numpy as np
from random import randint
a = randint(0,10)
print(f"exp({a}) = {np.exp(a)}")
```

Estás convencido que es un programa impresionante (después de todo, ¡Calcula $e$ a la potencia de un entero aleatorio), y que otros serán 
capaces de usarlo. Y no quieres molestarlos configurando Python, instalando numpy, y luego ejecutando el *script*. De hecho, ni siquiera 
necesitan saber que es un programa Python. Podrías haberlo escrito también en R, o en cualquier otro lenguaje (de cara al usuario no 
habría ninguna diferencia).

¿Qué tendría que contener una imagen Docker que ejecute este código? No mucho: algún sistema operativo básico (normalmente, una pequeña 
distribución de Linux), Python, 'numpy', y el propio *script*.

Para crear la imagen Docker, crea un archivo llamado 'Dockerfile', en el mismo directorio que tu *script*, con el siguiente contenido:

```
FROM python:3
ADD myscript.py /
RUN pip install numpy
CMD [ "python", "./myscript.py" ]
```

La primera línea le dice a Docker que construya una nueva imagen, partiendo de una imagen ya existente que contiene un sistema operativo 
y Python3. Podrías empezar desde cero, pero esto simplifica el proceso. La siguiente línea añade tu *script* a la imagen, y luego 
ejecutamos 'pip install numpy' dentro de la imagen. La última línea solo especifica qué comando con qué parámetros necesita ser ejecutado 
cuando se ejecuta la imagen (en nuestro caso 'python ./myscript.py').

Para crear la imagen, ejecuta 'docker build -t dockertest .' (nombrando la imagen "'dockertest'"). Después, puedes ejecutarla con 'docker 
run dockertest' y, si quieres, publicarla.

¿Fácil, verdad?

Pero, ¿cuándo tiene sentido utilizar Docker? No en nuestro *toy example*, por supuesto. Mientras que el código original es solo un par de 
bytes, ahora se ha hinchado a cientos de megabytes. Pero hay muchos escenarios en los que sí resulta útil.

-  Para "abstraer" el funcionamiento interno del código. En lugar de dar instrucciones potencialmente complicadas sobre cómo ejecutar tu código, qué dependencias instalar… etc., puedes proporcionar a los usuarios la imagen Docker, en la que todo está ya resuelto.

-  Para garantizar que los usuarios obtengan los mismos resultados. Aunque no constituye un gran problema en el día a día de la mayoría de los científicos computacionales, las diferentes versiones de diferentes bibliotecas en diferentes sistemas pueden ocasionalmente producir resultados ligeramente diferentes. El contenedor garantiza que el código se ejecute utilizando la misma configuración de software.

-  Para evitar interferir con las instalaciones existentes. Nuestro *toy example* ya tenía una dependencia ('numpy'), pero, a menudo, las dependencias son más complejas, se pueden necesitar librerías muy específicas, o incluso algún otro software más allá de las librerías Python o R. Distribuir solo el código fuente significa obligar al usuario a instalarlas también; y hay muchas razones por las que la gente puede ser reacia a hacerlo. Puede ser incompatible con otro software de su ordenador, puede haber problemas de seguridad, o simplemente es demasiado trabajo. Pero si se ejecuta dentro del contenedor Docker, muchos de estos problemas desaparecen.

En resumen, la imagen Docker nunca es la única forma de distribuir tu código fuente. Pero añadir un Dockerfile a tu repositorio de GitHub 
para que los usuarios puedan construir un contenedor Docker puede ofrecer otra forma, quizá mejor, de hacer tu software accesible a tu 
público.


[^1]: A veces, esta es una razón para evitar los marcos de datos: por ejemplo, es posible utilizar un generador que lea los datos línea por línea desde un archivo y los ceda a 'scikit-learn'. De esta manera, solo una fila de datos está en la memoria cada vez (ver Sección [-@sec-functions]).︎

[^2]: Échale un ojo a [compendium.ccs.amsterdam]([https://compendium.ccs.amsterdam])


```{bash}
#| echo: false
rm -f mydb.db mydb.sqlite
```