# Primeros pasos: juguetear con datos y gráficos {#sec-chap-fundata}

```{python}
#| echo: false
import warnings; warnings.filterwarnings('ignore')
```

**Resumen.**
  
Este capítulo es una visita relámpago a algunas de las cosas interesantes (e instructivas) 
que se pueden hacer con R y Python. Partiendo de una base de datos de tuits sobre COVID-19, 
mostraremos cómo puedes realizar análisis utilizando análisis de texto, de redes y utilizando 
información geográfica. El objetivo de este capítulo no es enseñarte todas estas técnicas 
en detalle, sino que cada uno de los ejemplos muestre una posibilidad y te guíe al capítulo 
donde se explicará en detalle. Así que no te preocupes demasiado por entender cada línea 
código, relájate ¡y disfruta del viaje!

**Palabras clave.** fundamentos de la programación, análisis de datos

**Objetivos:**

- Obtener una visión general de las posibilidades de R y Python para el análisis y visualización de datos.
 - Comprender cómo funcionan juntos los diferentes aspectos de la recopilación, limpieza y análisis de datos
 - ¡Divertirse con los datos y las visualizaciones!


::: {.callout-note icon=false collapse=true}
## Paquetes utilizados en el capítulo

Como este capítulo muestra una gran cantidad de posibilidades, necesita unos cuantos paquetes de terceros. 
Si aún no los tienes, puedes instalarlos con el código que hay a continuación 
(ve a la sección [-@sec-installing] para más detalles):

::: {.panel-tabset}
## Código Python
```{python chapter02install-python}
#| eval: false
!pip3 install pandas matplotlib  geopandas 
!pip3 install descartes shifterator
!pip3 install wordcloud gensim nltk networkx
```
## Código R
```{r chapter02install-r}
#| eval: false
install.packages(c("tidyverse", "igraph","maps",
    "quanteda", "quanteda.textplots", 
    "quanteda.textstats", "topicmodels"))
```
:::
Una vez instalados, es necesario importar (activar) los paquetes en cada sesión:

::: {.panel-tabset}
## Código Python
```{python chapter02library-python}
import re
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
from wordcloud import WordCloud
from gensim import corpora, models
import geopandas as gpd
import shifterator as sh
import nltk
from nltk.corpus import stopwords
import networkx as nx

```
## Código R
```{r chapter02library-r}
library(tidyverse)
library(lubridate)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(topicmodels)
library(igraph)
library(maps)
```
:::
:::

## Divirtámonos con tuits {#sec-funtweets}

El objetivo de este capítulo es mostrar cómo se puede utilizar R o Python para ejecutar 
rápida y fácilmente impresionantes análisis de datos del mundo real. Para ello, 
utilizaremos una base de datos de tuits sobre la pandemia de COVID que tanto afectó 
a nuestro mundo. Por supuesto, es probable que los tuits solo sean representativos 
de lo que se dice en Twitter, pero los datos son (semi)públicos y ricos, ya que 
contienen texto, ubicación y características de red. Esto los hace ideales para 
explorar las muchas formas en que podemos analizar y visualizar la información 
con Python y R.

El ejemplo [-@exm-funtweets] muestra cómo se puede leer esta base de datos en memoria 
utilizando un único comando. Nótese que esto no recupera los tuits del propio Twitter, 
sino que descarga nuestra versión en caché de los tuits. En el [-@sec-chap-scraping] mostraremos 
cómo puedes descargar los tuits y los datos de localización por ti mismo, pero, para 
asegurarnos de que podemos ponernos manos a la obra inmediatamente, empezaremos desde 
esta versión en caché.


::: {.callout-note appearance="simple" icon=false}
::: {#exm-funtweets}
Recuperando tuits en caché sobre el COVID.

::: {.panel-tabset}
## Código Python
```{python tweets-python}
tw = pd.read_csv("https://cssbook.net/d/covid.csv")
tw.head()

```
## Código R
```{r tweets-r}
tw = read_csv("https://cssbook.net/d/covid.csv")
head(tw)
```
:::
:::
:::

Como puedes ver, la base de datos cuenta con casi 10.000 tuits, con su remitente (autor), 
ubicación e idioma, el texto, el número de retuits y si se trata de una respuesta. 
Se puede leer el inicio de los tres mensajes más retuiteados, siendo un tuit (político) 
de la India y dos tuits aparentemente políticos y con datos de Estados Unidos.

**Mi primer gráfico de barras.** Antes de sumergirnos en los datos textuales, de red y 
geográficos, hagamos una sencilla visualización de la fecha en la que se publicaron 
los tuits. El ejemplo [-@exm-funtime] lo hace en dos pasos: primero, cuenta el número de tuits por 
hora con un comando de agregación. A continuación, hace un gráfico de barras de este 
valor calculado, pidiendo algunas condiciones para que tenga un aspecto relativamente 
limpio y profesional. Si quieres jugar con esto, puedes, por ejemplo, intentar representar 
el número de tuits por idioma, o crear un gráfico de líneas en lugar de un gráfico de 
barras. Para más información sobre visualización, consulta el Capítulo [-@sec-chap-eda]. Ve al Capítulo [-@sec-chap-datawrangling] 
para una explicación detallada del comando de agregación.


::: {.callout-note appearance="simple" icon=false}

::: {#exm-funtime}
Gráfico de barras de los tuits a lo largo del tiempo

::: {.panel-tabset}
## Código Python
```{python funtime-python}
#| results: hide
tw.index = pd.DatetimeIndex(tw["created_at"])
tw["status_id"].groupby(pd.Grouper(freq="H")).count().plot(kind="bar")
# (fíjate en el uso de \ para dividir una línea demasiado larga)

```
## Código R
```{r funtime-r}
tweets_per_hour = tw %>% 
  mutate(hour=round_date(created_at, "hour")) %>% 
  group_by(hour) %>% summarize(n=n()) 
ggplot(tweets_per_hour, aes(x=hour, y=n)) + 
  geom_col() + theme_classic() + 
  xlab("Time") + ylab("# of tweets") +
  ggtitle("Number of COVID tweets over time")
```
:::
:::
:::

## Divirtámonos con datos de texto {#sec-funtext}

**Análisis del corpus.** A continuación, podemos analizar qué hashtags se utilizan con más frecuencia 
en esta base de datos. El ejemplo [-@exm-funcloud] lo hace creando una *document-term matrix* 
(matriz término-documento) con el paquete `quanteda` (en R) o contando manualmente las palabras con 
`defaultdict` (en Python). El código muestra la serie de pasos que se realizan para crear los resultados 
finales, cada uno de los cuales representa decisiones del investigador sobre qué datos conservar y 
cuáles descartar como ruido. En este caso, seleccionamos los tuits en inglés, convertimos el texto a 
minúsculas, eliminamos las *stop words* y conservamos solo las palabras que empiezan con un \#, mientras 
que descartamos las palabras que empiezan por `#corona` y `#covid`. Para jugar con este ejemplo, 
comprueba si puedes ajustar el código para, por ejemplo, incluir todas las palabras o solo las menciones 
en lugar de los hashtags y hacer una selección diferente de tuits, por ejemplo, tuits en español o solo 
tuits populares (retuiteados). Consulta el capítulo [-@sec-chap-dtm] si quieres obtener más información 
sobre el análisis de corpus, y consulta el capítulo [-@sec-chap-datawrangling] para obtener más información 
sobre cómo seleccionar subconjuntos de datos.


::: {.callout-note appearance="simple" icon=false}

::: {#exm-funcloud}
Mi primera nube de etiquetas 

::: {.panel-tabset}
## Código Python
```{python funcloud-python}
#| results: hide
freq = defaultdict(int)
for tweet in tw["text"]:
    for tag in re.findall("#\w+", tweet.lower()):
        if not re.search("#covid|#corona", tag):
            freq[tag] += 1
wc = WordCloud().generate_from_frequencies(freq)
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")

```
## Código R
```{r funcloud-r}
dtm_tags = filter(tw, lang=="en") %>% 
  corpus() %>% tokens() %>% 
  dfm(tolower = T) %>% 
  dfm_select(pattern = "#*") %>% 
  dfm_remove(c("#corona*", "#covid*")) 
textplot_wordcloud(dtm_tags, max_words=100)
```
:::
:::
:::

**Topic model o modelado de temas** Mientras que una nube de palabras (o nube de etiquetas) muestra 
qué palabras aparecen con más frecuencia, un `topic model` muestra qué palabras coinciden en los 
mismos documentos. El Ejemplo [-@exm-funlda] utiliza la Asignación Latente de Dirichlet (Latent 
Dirichlet Allocation o LDA, por sus siglas en inglés), el algoritmo de análisis de *topic modeling* 
más común. Con este algoritmo, se exploran los tuits agrupando automáticamente los *hashtags* 
anteriormente identificados en 10 *topics* (temas). Esta técnica no es determinista: si lo ejecutas 
de nuevo, puede que obtengas temas ligeramente diferentes. Además, estos temas se intercambian 
aleatoriamente, ya que su numeración no tiene ningún significado en especial. Introduciendo una 
*random seed*, es posible asegurarse de que, al ejecutarse de nuevo, se obtendrán los mismos resultados. 
Como puedes ver, algunos temas son fáciles de interpretar (como el tema 2 sobre distanciamiento social 
y el tema 8 sobre asistencia sanitaria), pero siempre se recomienda examinar los documentos agrupados 
y los casos extremos, además de las palabras (o etiquetas) principales, como se muestra aquí. Puedes 
jugar con este ejemplo utilizando una selección diferente de palabras (modificando el código del 
Ejemplo [-@exm-funcloud]) o cambiando el número de temas. También puedes cambiar (o eliminar) la 
*random seed* y ver cómo al ejecutar el mismo modelo varias veces se obtienen resultados diferentes. 
Consulta la @sec-unsupervised para obtener más información sobre el ajuste, la interpretación y la 
validación del modelado de temas.


::: {.callout-note appearance="simple" icon=false}

::: {#exm-funlda}
Modelado de temas de los hashtags sobre el COVID

::: {.panel-tabset}
## Código Python
```{python funlda-python}
tags = [
    [tag.lower() for tag in re.findall("#\w+", tweet)] for tweet in tw["text"]
]
voca = corpora.Dictionary(tags)
corpus = [voca.doc2bow(doc) for doc in tags]
m = models.LdaModel(
    corpus, num_topics=10, id2word=voca, distributed=False, random_state=123
)
for topic, words in m.print_topics(num_words=3):
    print(f"{topic}: {words}")

```
## Código R
```{r funlda-r}
set.seed(1)
m = convert(dtm_tags, to="topicmodel") %>% 
  LDA(10, method="Gibbs")
terms(m, 5)
```
:::
:::
:::

## Divirtámonos visualizando información geográfica {#sec-fungeo}
Para la última serie de ejemplos, vamos a utilizar la información de localización contenida en 
los datos de Twitter. Esta información se basa en lo que los usuarios de Twitter introducen en 
su perfil, por lo que es incompleta y ruidosa, ya que muchos usuarios dan una ubicación sin 
sentido como *"Ethereally here"*  ("Por aquí") o no rellenan ninguna ubicación en absoluto. 
Sin embargo, si asumimos que la mayoría de los usuarios que introducen una ubicación correcta 
(como Lahore o Florida en los primeros tuits mostrados más arriba), dan su ubicación real, 
podemos utilizarla para determinar de dónde proceden la mayoría de los tuits.

El primer paso en este análisis es convertir un nombre como "Lahore, Pakistán" en sus coordenadas 
geográficas (en este caso, unos 31 grados norte y 74 grados este). Esto se denomina geocodificación, 
y tanto Google Maps como Open Street Maps pueden utilizarse para realizarla automáticamente. Al igual 
que con los propios tuits, aquí utilizaremos una versión en caché de los resultados de la geocodificación 
para poder proceder directamente. En https://cssbook.net/datasets encontrarás el código que se ha 
utilizado para crear este archivo para que puedas jugar con él.

El ejemplo [-@exm-funmap] muestra cómo se pueden utilizar estos datos para crear un mapa de la 
actividad en Twitter. En primer lugar, se recuperan los datos de usuario almacenados en caché, 
que muestran la ubicación correcta de Lahore, pero también ilustran el ruido de los datos con 
la ubicación *"Un peu partout"* (un poco en todas partes). A continuación, estos datos se unen a 
los de Twitter, de modo que las coordenadas se rellenan cuando es posible. Por último, representamos 
esta información en un mapa, mostrando los tuits con más retuits como puntos más grandes. Para 
más información sobre la visualización, consulta el capítulo [-@sec-chap-eda].

::: {.callout-note appearance="simple" icon=false}

::: {#exm-funmap}
Localización de los tuits sobre el COVID

::: {.panel-tabset}
## Código Python
```{python funmap-python}
#| results: hide
url = "https://cssbook.net/d/covid_users.csv"
users = pd.read_csv(url)
tw2 = tw.merge(users, on="screen_name", how="left")
world = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
gdf = gpd.GeoDataFrame(tw2, geometry=gpd.points_from_xy(tw2.long, tw2.lat))
ax = world.plot(color="white", edgecolor="black", figsize=(10, 10))
gdf.plot(ax=ax, color="red", alpha=0.2, markersize=tw["retweet_count"])
plt.show()

```
## Código R
```{r funmap-r}
url = "https://cssbook.net/d/covid_users.csv"
users = read_csv(url)
tw2 = left_join(tw, users)
ggplot(mapping=aes(x=long, y=lat)) +
  geom_polygon(aes(group=group), 
    data=map_data("world"), 
    fill="lightgray", colour = "white") +
  geom_point(aes(size=retweet_count, 
                 alpha=retweet_count), 
             data=tw2, color="red") + 
  theme_void() + theme(aspect.ratio=1) + 
  guides(alpha=FALSE, size=FALSE) + 
  ggtitle("Location of COVID tweets", 
          "Size indicates number of retweets")
```
:::
:::
:::

**Combinación de información textual e información estructurada.**
Dado que conocemos la ubicación de un subconjunto de los usuarios, 
podemos diferenciar, por ejemplo, entre tuits norteamericanos, europeos y 
asiáticos. El ejemplo [-@exm-funcompare] crea una identificación muy aproximada 
de los tuits norteamericanos y la utiliza para calcular la frecuencia relativa 
de las palabras en esos tuits en comparación con el resto. No es sorprendente 
que esos tuits traten mucho más sobre política, lugares e instituciones 
estadounidenses. Los demás tuits hablan de política británica, pero también 
utilizan diversos nombres para referirse a la pandemia. Para jugar con esto, 
comprueba si puedes aislar, por ejemplo, tuits asiáticos o latinoamericanos, 
o compara los tuits en español de diferentes lugares.

::: {.callout-note appearance="simple" icon=false}

::: {#exm-funcompare}
Comparación del corpus: los tuits norteamericanos vs. los demás.

::: {.panel-tabset}
## Código Python
```{python funcompare-python}
#| results: hide
nltk.download("stopwords")
cn = gdf.query("lang=='en'&(long<-60 & lat>25)")
cn = Counter(cn["text"].str.cat().lower().split())
cr = gdf.query("lang=='en' & (long>-60 | lat<25)")
cr = Counter(cr["text"].str.cat().lower().split())
for k in stopwords.words("english"):
    del cn[k]
    del cr[k]
key = sh.ProportionShift(type2freq_1=cn, type2freq_2=cr)
key.get_shift_graph().plot()

```
## Código R
```{r funcompare-r}
dfm = tw2 %>% mutate(northamerica=ifelse(
    long < -60 & lat > 25,"N. America","Rest"))%>%
  filter(lang=="en") %>% 
  corpus(docid_field="status_id") %>% 
  tokens(remove_punct=T) %>%
  tokens_group(northamerica) %>%
  dfm(tolower=T) %>% 
  dfm_remove(stopwords("en")) %>%
  dfm_select(min_nchar=4)
key = textstat_keyness(dfm, target="N. America")
textplot_keyness(key, margin=0.2) +
  ggtitle("Words preferred by North Americans", 
          "(Only English-language tweets)") + 
  theme_void()

```
:::
:::
:::

## Divirtámonos con redes {#sec-funnet}

Twitter, por supuesto, es una red social además de un servicio de *microblogging*: 
los usuarios están conectados con otros usuarios porque se siguen unos a otros y 
retuitean y les gustan los tuits de los demás. Utilizando la columna 'reply_to_screen_name', 
podemos inspeccionar la red de respuestas contenida en la base de datos de tuits de COVID. 
El ejemplo [-@exm-fungraph] utiliza primero los comandos de resumen de datos de 'tidyverse' (R) 
y 'pandas' (Python) para crear un marco de datos de conexiones o *edges* (aristas) que enumera 
la frecuencia con la que cada usuario responde a otro usuario. El segundo bloque de código 
muestra cómo se utilizan los paquetes 'igraph' (R) y 'networkx' (Python) para convertir esta 
lista de aristas en un gráfico. A partir de este gráfico, seleccionamos solo el componente 
conectado más grande y utilizamos un algoritmo de agrupación para analizar qué nodos (usuarios) 
forman subredes cohesionadas. Por último, se utilizan varias opciones para establecer el color 
y el tamaño de las aristas, los nodos y las etiquetas, y se representa gráficamente la red 
resultante. Como puedes ver, el nodo central es Donald Trump, a quién responden un gran 
número de usuarios, algunos de los cuales son respondidos por otros usuarios. Puedes jugar 
con diferentes opciones de trazado o intentar filtrar, por ejemplo, solo los tuits de un 
determinado idioma. También puedes calcular fácilmente métricas de redes sociales como la 
centralidad en esta red y/o exportar la red para su posterior análisis en software especializado 
en análisis de redes sociales. Ve al Capítulo [-@sec-chap-network] para más información sobre el 
análisis de redes, y al Capítulo [-@sec-chap-datawrangling] para los comandos de resumen 
utilizados para crear la lista de aristas.

::: {.callout-note appearance="simple" icon=false}
::: {#exm-fungraph}
Red de respuestas en los tuits sobre el COVID .

::: {.panel-tabset}
## Código Python
```{python fungraph-python}
edges = tw2[["screen_name", "reply_to_screen_name"]]
edges = edges.dropna().rename(
    {"screen_name": "from", "reply_to_screen_name": "to"}, axis="columns"
)
edges.groupby(["from", "to"]).size().head()

```
## Código R
```{r fungraph-r}
edges = tw2 %>% 
  select(from=screen_name, 
         to=reply_to_screen_name) %>% 
  filter(to != "") %>%
  group_by(to, from) %>% 
  summarize(n=n())
head(edges)
```
:::

::: {.panel-tabset}
## Código Python
```{python fungraphb-python}
#| results: hide
g1 = nx.Graph()
g1.add_edges_from(edges.to_numpy())
largest = max(nx.connected_components(g1), key=len)
g2 = g1.subgraph(largest)

pos = nx.spring_layout(g2)
plt.figure(figsize=(20, 20))
axes_info = plt.axis("off")
sizes = [s * 1e4 for s in nx.centrality.degree_centrality(g2).values()]
nx.draw_networkx_nodes(g2, pos, node_size=sizes)
edge_info = nx.draw_networkx_labels(g2, pos)
nx.draw_networkx_edges(g2, pos)
plt.show()

```
## Código R
```{r fungraphb-r}
# Crea un igraph y selecciona el componente más grande
g = graph_from_data_frame(edges)
components <- decompose.graph(g)
largest = which.max(sapply(components, gsize))
g2 = components[[largest]]
# Colorea los nodos según el clúster al que pertenecen
clusters = cluster_spinglass(g2)
V(g2)$color = clusters$membership
V(g2)$frame.color = V(g2)$color
# Asigna un tamaño a los nodos (el usuario) y las aristas (las flechas)
V(g2)$size = degree(g2)^.5
V(g2)$label.cex = V(g2)$size/3
V(g2)$label = ifelse(degree(g2)<=1,"",V(g2)$name) 
E(g2)$width = E(g2)$n
E(g2)$arrow.size= E(g2)$width/10
plot(g2)
```
:::
:::
:::

**Redes geográficas.**
En el último ejemplo de este capítulo, combinaremos la información geográfica y de red para 
mostrar qué regiones del mundo interactúan entre sí. Para ello, en el Ejemplo [-@exm-fungeonet] 
vamos a unir dos veces la información del usuario a la base de datos de aristas creada anteriormente: 
una para el remitente y otra para el usuario al que se responde. A continuación, adaptamos 
el código anterior para trazar el mapa añadiendo una línea para cada nodo de la red. 
Como puede verse, los usuarios de las principales regiones (EE.UU., UE, India) interactúan 
mayoritariamente entre sí, y casi todas las regiones interactúan también con EE.UU.

::: {.callout-note appearance="simple" icon=false}

::: {#exm-fungeonet}
Red de respuestas de los tuits

::: {.panel-tabset}
## Código Python
```{python fungeonet-python}
#| results: hide
u = users.drop(["location"], axis=1)
uf = u.rename(
    {"screen_name": "from", "lat": "lat_from", "long": "long_from"}, axis=1
)
ut = u.rename({"screen_name": "to", "lat": "lat_to", "long": "long_to"}, axis=1)
edges = edges.merge(uf).merge(ut).query("long_to!=long_from & lat_to!=lat_from")

world = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
g_to = gpd.GeoDataFrame(
    edges.copy(), geometry=gpd.points_from_xy(edges.long_to, edges.lat_to)
)
g_from = gpd.GeoDataFrame(
    edges.copy(), geometry=gpd.points_from_xy(edges.long_from, edges.lat_from)
)

ax = world.plot(color="white", edgecolor="black", figsize=(10, 10))
g_from.plot(ax=ax, color="red", alpha=0.2)
g_to.plot(ax=ax, color="blue", alpha=0.2)

e = g_from.join(g_to, lsuffix="_from", rsuffix="_to")
e = e[["geometry_from", "geometry_to"]]
px = lambda point: point.x
py = lambda point: point.y

# WVA: El código ya no funciona, pero devuelve:
#      UnsupportedOperationException: getX called on empty Point
# x_values = list(zip(e["geometry_from"].map(px),
#                    e["geometry_to"].map(px)))
# y_values = list(zip(e["geometry_from"].map(py),
#                    e["geometry_to"].map(py)))
# plt.plot(x_values, y_values, linewidth = 1,
#    linestyle = "-", color = "green", alpha=.3)
# plt.show()

```
## Código R
```{r fungeonet-r}
edges2 = edges %>% 
  inner_join(users, by=c("from"="screen_name"))%>%
  inner_join(users, by=c("to"="screen_name"), 
             suffix=c("", ".to")) %>% 
  filter(lat != lat.to | long != long.to )
ggplot(mapping=aes(x = long, y = lat)) +
  geom_polygon(aes(group=group),map_data("world"),
    fill="lightgray", colour = "white") +
  geom_point(aes(size=retweet_count, 
  alpha=retweet_count), data=tw2, color="red")+
  geom_curve(aes(xend=long.to,yend=lat.to,size=n),
             edges2, curvature=.1, alpha=.5) +
  theme_void() + guides(alpha=FALSE, size=FALSE) +
  ggtitle("Retweet network of COVID tweets", 
  "Bubble size indicates total no. of retweets")
```
:::
:::
:::

